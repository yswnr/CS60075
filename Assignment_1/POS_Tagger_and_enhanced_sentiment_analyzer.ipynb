{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **POS Tagger Using viterbi Algorithm**"
      ],
      "metadata": {
        "id": "B7n-cg7g0NTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "import nltk, re, pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pprint, time\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Dowloading treebank library with universal tags\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "corpus = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
        "from tqdm import tqdm\n",
        "# Getting list of tagged words\n",
        "tagged_words = [tup for sent in corpus for tup in sent]\n",
        "# tokens(words)\n",
        "tokens = [pair[0] for pair in tagged_words]\n",
        "# vocabulary\n",
        "V = set(tokens)\n",
        "# tags\n",
        "T = set([pair[1] for pair in tagged_words])\n",
        "# computing P(w/t)(Emmision_probabilities) and storing in T x V matrix\n",
        "t = len(T)\n",
        "v = len(V)\n",
        "w_given_t = np.zeros((t, v))\n",
        "def word_given_tag(word, tag, train_bag = tagged_words):\n",
        "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
        "    count_tag = len(tag_list)\n",
        "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
        "    count_w_given_tag = len(w_given_tag_list)\n",
        "\n",
        "    return (count_w_given_tag, count_tag)\n",
        "# computing tag given tag: tag2(t2) given tag1 t1), Transition Probability\n",
        "\n",
        "def t2_given_t1(t2, t1, train_bag = tagged_words):\n",
        "    tags = [pair[1] for pair in train_bag]\n",
        "    count_t1 = len([t for t in tags if t==t1])\n",
        "    count_t2_t1 = 0\n",
        "    for index in range(len(tags)-1):\n",
        "        if tags[index]==t1 and tags[index+1] == t2:\n",
        "            count_t2_t1 += 1\n",
        "    return (count_t2_t1, count_t1)\n",
        "# creating T x T transition matrix of tags\n",
        "\n",
        "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
        "for i, t1 in enumerate(list(T)):\n",
        "    for j, t2 in enumerate(list(T)):\n",
        "        tags_matrix[i, j] = (0.000001+t2_given_t1(t2, t1)[0])/(t2_given_t1(t2, t1)[1]+t*0.000001)    # to avoid zeroes we do smoothing\n",
        "# convert the matrix to a dataframe\n",
        "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))\n",
        "train_bag = tagged_words\n",
        "T = list(set([pair[1] for pair in train_bag]))\n",
        "W = list(set([pair[0] for pair in train_bag]))\n",
        "emmission_matrix = np.zeros((len(W),len(T)))\n",
        "emmission_dict = pd.DataFrame(emmission_matrix, columns = T, index=W)\n",
        "w_size = len(W)\n",
        "def emp(word,tag):\n",
        "  return (word_given_tag(word, tag)[0]+0.000001)/(word_given_tag(word, tag)[1]+w_size*0.000001)      # to avoid zeroes smoothing is done\n",
        "for word in W:\n",
        "  for tag in T:\n",
        "    emmission_dict.loc[word,tag] = emp(word,tag)\n"
      ],
      "metadata": {
        "id": "X8yPDG9Kt6C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Viterbi(words):\n",
        "    viterbi = []          # for each word it stores the best tag\n",
        "\n",
        "    for key, word in enumerate(words):\n",
        "        p = []\n",
        "        for tag in T:\n",
        "            if key == 0:\n",
        "                transition_p = tags_df.loc['.', tag]                           # '.' is considered the start symbol\n",
        "            else:\n",
        "                transition_p = tags_df.loc[viterbi[-1], tag]                   # transition is considered from the previous best tag to the current tag\n",
        "\n",
        "            # compute emission and state probabilities\n",
        "            if word in W:\n",
        "              emission_p = emmission_dict.loc[word,tag]\n",
        "            else:\n",
        "              emission_p = 0.000001                                            # To tackle words that are not there in the vocabulary of the trained dataset we assign a small value to their emmission probability\n",
        "            viterbi_probability = emission_p * transition_p\n",
        "            p.append(viterbi_probability)\n",
        "\n",
        "        pmax = max(p)                                                          # finding max probability to store the best tag\n",
        "        # getting state for which probability is maximum\n",
        "        best_tag = T[p.index(pmax)]\n",
        "        viterbi.append(best_tag)\n",
        "    return list(zip(words, viterbi))"
      ],
      "metadata": {
        "id": "x6UFZu4V-xwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Enhanced Sentiment Analyzer***"
      ],
      "metadata": {
        "id": "Q3q1xx4X_Z2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import movie_reviews\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Downloading the movie_reviews corpus\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "\n",
        "# Loading movie_reviews corpus documents and labels\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)  # Shuffling the documents\n",
        "train_set, test_set = train_test_split(documents, test_size=0.2, random_state=42)\n",
        "train_set, val_set = train_test_split(train_set, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "# creating embeddings of words using word2vec\n",
        "model = Word2Vec([doc for doc, _ in train_set], vector_size=100)\n",
        "\n",
        "\n",
        "# embedding on adverbs and adjectives for enhanced sentiment analysis\n",
        "def filter(doc):\n",
        "  words =[]\n",
        "  for word,tag in Viterbi(doc):\n",
        "    if tag == 'ADJ' or tag =='ADV':\n",
        "      words.append(word)\n",
        "  return words\n",
        "\n",
        "\n",
        "# function to create document embeddings\n",
        "def document_embedding(doc, model):\n",
        "    words = [word for word in doc if word in model.wv] #creating a list\n",
        "    if not words:        return None\n",
        "    return sum(model.wv[word] for word in words) / len(words)\n",
        "\n",
        "\n",
        "# Creating feature sets\n",
        "X_train = [document_embedding(filter(doc), model) for doc, _ in tqdm(train_set)]\n",
        "X_val = [document_embedding(filter(doc), model) for doc, _ in val_set]\n",
        "X_test = [document_embedding(filter(doc), model) for doc, _ in test_set]\n",
        "\n",
        "# Extracting labels\n",
        "y_train = [category for _, category in train_set]\n",
        "y_val = [category for _, category in val_set]\n",
        "y_test = [category for _, category in test_set]\n",
        "\n",
        "# Removing None values (documents that couldn't be embedded)\n",
        "X_train = [embedding for embedding in X_train if embedding is not None]\n",
        "X_val = [embedding for embedding in X_val if embedding is not None]\n",
        "X_test = [embedding for embedding in X_test if embedding is not None]\n",
        "y_train = y_train[:len(X_train)]\n",
        "y_val = y_val[:len(X_val)]\n",
        "y_test = y_test[:len(X_test)]\n",
        "\n",
        "\n",
        "# Training an SVM classifier\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluating the classifier on the validation set\n",
        "val_predictions = svm_classifier.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, val_predictions)\n",
        "print(f'Validation Accuracy: {accuracy_val:.2f}')\n",
        "print(classification_report(y_val, val_predictions))\n",
        "\n",
        "# Evaluating the classifier on the test set\n",
        "test_predictions = svm_classifier.predict(X_test)\n",
        "accuracy_test = accuracy_score(y_test, test_predictions)\n",
        "print(f'Test Accuracy: {accuracy_test:.2f}')\n",
        "print(classification_report(y_test, test_predictions))"
      ],
      "metadata": {
        "id": "vI_0ye2W_k9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}