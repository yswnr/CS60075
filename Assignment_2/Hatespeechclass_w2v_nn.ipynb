{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting drive"
      ],
      "metadata": {
        "id": "eNhaXQZsfoSQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrApVArteAXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f544bb2-004c-4e3f-c0db-3f497f52223e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "dIcLR1TCf-Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desired_directory = '/content/drive/MyDrive'\n",
        "\n",
        "os.chdir(desired_directory)\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current Working Directory:\", current_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzif5xLPgGB3",
        "outputId": "b86e84a3-ee68-40be-d966-b36d85e848cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd NLP_ass_2_3"
      ],
      "metadata": {
        "id": "Xeugr1tOgIaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b288f869-8dfd-43e8-c034-c61ed9bd3d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP_ass_2_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing Data"
      ],
      "metadata": {
        "id": "flOtW6A8gPrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "8TBZlrFqgR7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78072866-aab3-444d-e1be-60ce2a045250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model"
      ],
      "metadata": {
        "id": "4bCNGgTMTMtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "metadata": {
        "id": "4W54dR3dVS1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path = '/content/drive/MyDrive/NLP_ass_2_3/GoogleNews-vectors-negative300.bin.gz'\n",
        "model = KeyedVectors.load_word2vec_format(path, binary=True)"
      ],
      "metadata": {
        "id": "ML0SGZ_YVRxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input feature generation"
      ],
      "metadata": {
        "id": "HgN6ojVC0Ow0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Load the TSV file into a DataFrame\n",
        "\n",
        "data = pd.read_csv('NLP_ass_train.tsv',delimiter = '\\t',header = None)\n",
        "text_column = data.columns[0]\n",
        "text_data = data[text_column].tolist()\n",
        "\n",
        "labels_column = data.columns[1]\n",
        "labels_list = data[labels_column].tolist()\n",
        "for i in range(len(labels_list)):\n",
        "  if labels_list[i] == \"normal\":\n",
        "    labels_list[i] = 0\n",
        "  elif labels_list[i] == \"hatespeech\":\n",
        "    labels_list[i] = 1\n",
        "  else:\n",
        "    labels_list[i] = 2\n",
        "\n",
        "def generate_input_features(comment, model):\n",
        "\n",
        "    preprocessed_comment = preprocess_text(comment)\n",
        "    #print(preprocessed_comment)\n",
        "\n",
        "    feature_vector = np.zeros(model.vector_size)\n",
        "\n",
        "\n",
        "    word_count = 0\n",
        "\n",
        "\n",
        "    for word in preprocessed_comment:\n",
        "        if word in model:\n",
        "            feature_vector += model[word]\n",
        "            word_count += 1\n",
        "\n",
        "\n",
        "    if word_count > 0:\n",
        "        feature_vector /= word_count\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "input_features = []\n",
        "for comment in text_data:\n",
        "    input_feature = generate_input_features(comment, model)\n",
        "    input_features.append(input_feature)\n",
        "\n",
        "\n",
        "input_features = np.array(input_features)\n",
        "#input_features[1]\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y, transform=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_x = self.x[idx]\n",
        "        sample_y = self.y[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            sample_x, sample_y = self.transform(sample_x, sample_y)\n",
        "\n",
        "        return sample_x, sample_y\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, width, depth):\n",
        "        super(DNN, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.layers = nn.ModuleList([nn.Linear(dim_in if i == 0 else width, width,bias=True) for i in range(self.depth)])\n",
        "        self.output_layer = nn.Linear(width, dim_out)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = self.relu(layer(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "data2 = pd.read_csv('NLP_ass_test.tsv',delimiter = '\\t',header = None)\n",
        "text2_column = data2.columns[0]\n",
        "text2_data = data2[text2_column].tolist()\n",
        "\n",
        "labels_column2 = data2.columns[1]\n",
        "labels_list2 = data2[labels_column2].tolist()\n",
        "for i in range(len(labels_list2)):\n",
        "  if labels_list2[i] == \"normal\":\n",
        "    labels_list2[i] = 0\n",
        "  elif labels_list2[i] == \"hatespeech\":\n",
        "    labels_list2[i] = 1\n",
        "  else:\n",
        "    labels_list2[i] = 2\n",
        "\n",
        "input_features2 = []\n",
        "for comment in text2_data:\n",
        "    input_feature2 = generate_input_features(comment, model)\n",
        "    input_features2.append(input_feature2)\n",
        "\n",
        "\n",
        "input_features2 = np.array(input_features2)\n",
        "\n",
        "data3 = pd.read_csv('NLP_ass_valid.tsv',delimiter = '\\t',header = None)\n",
        "text3_column = data3.columns[0]\n",
        "text3_data = data3[text3_column].tolist()\n",
        "\n",
        "labels_column3 = data3.columns[1]\n",
        "labels_list3 = data3[labels_column3].tolist()\n",
        "for i in range(len(labels_list3)):\n",
        "  if labels_list3[i] == \"normal\":\n",
        "    labels_list3[i] = 0\n",
        "  elif labels_list2[i] == \"hatespeech\":\n",
        "    labels_list3[i] = 1\n",
        "  else:\n",
        "    labels_list3[i] = 2\n",
        "\n",
        "input_features3 = []\n",
        "for comment in text3_data:\n",
        "    input_feature3 = generate_input_features(comment, model)\n",
        "    input_features3.append(input_feature3)\n",
        "\n",
        "\n",
        "input_features3 = np.array(input_features3)\n",
        "\n",
        "\n",
        "x_train = torch.tensor(input_features)\n",
        "y_train = torch.tensor(labels_list)\n",
        "\n",
        "x_test = torch.tensor(input_features2)\n",
        "y_test = torch.tensor(labels_list2)\n",
        "\n",
        "x_valid = torch.tensor(input_features3)\n",
        "y_valid = torch.tensor(labels_list3)\n",
        "\n",
        "\n",
        "# Create DataLoaders for train and test datasets\n",
        "batch_size = 32\n",
        "\n",
        "# Assuming you have a custom Dataset class, you can create instances like this:\n",
        "train_dataset = CustomDataset(x_train, y_train)\n",
        "test_dataset = CustomDataset(x_test, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "dim_in = len(x_train[0])\n",
        "num_classes = 3\n",
        "\n",
        "model_dnn = DNN(dim_in,num_classes,512,3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model_dnn.parameters(), lr = learning_rate, momentum = 0.9)\n",
        "\n",
        "log_epochs = 10\n",
        "num_epochs = 500\n",
        "log_weight = 10\n",
        "\n",
        "model_dnn.to(device)\n",
        "Train_losses=[]\n",
        "Val_losses=[]\n",
        "best_valid_acc=0\n",
        "for epoch in range(num_epochs):\n",
        "    model_dnn.train()\n",
        "    for x_batch, y_batch in train_dataloader:\n",
        "        x_batch = x_batch.to(torch.float32).to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        pred = model_dnn(x_batch)\n",
        "        loss = loss_fn(pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    # if epoch % log_weight == 0:\n",
        "    #     features_train.append(log_features(model_dnn))\n",
        "    if epoch % log_epochs == 0:\n",
        "        loss_full = loss_fn(model_dnn(x_train.to(torch.float32).to(device)),y_train.to(device))\n",
        "        Train_losses.append(loss_full.item())\n",
        "        val_loss = loss_fn(model_dnn(x_valid.to(torch.float32).to(device)),y_valid.to(device))\n",
        "        valid_pred = torch.argmax(model_dnn(x_valid.to(torch.float32).to(device)), dim=1)\n",
        "        zero_mask = (valid_pred==y_valid.to(device))\n",
        "        valid_acc = zero_mask.sum().item()/len(y_valid)\n",
        "        best_val_acc = 0\n",
        "        if valid_acc > best_val_acc:\n",
        "          best_val_acc = valid_acc\n",
        "          torch.save(model_dnn.state_dict(), 'best_model_weights.pt')\n",
        "        print(f'Epoch {epoch} Train Loss {loss_full.item():.4f}  valid loss {val_loss.item():.4f}  valid acc {valid_acc}')\n",
        "    if loss_full.item() < 0.01:\n",
        "        print(f'Early stopping at epoch {epoch} because loss is below 0.01')\n",
        "        break\n",
        "\n",
        "model_dnn.load_state_dict(torch.load('best_model_weights.pt'))\n",
        "\n",
        "test_loss_full = loss_fn(model_dnn(x_test.to(torch.float32).to(device)),y_test.to(device))\n",
        "print(test_loss_full)\n",
        "\n",
        "train_pred = torch.argmax(model_dnn(x_train.to(torch.float32).to(device)), dim=1)\n",
        "zero_mask = (train_pred==y_train.to(device))\n",
        "\n",
        "# Count the number of zeros\n",
        "train_acc = zero_mask.sum().item()/len(y_train)\n",
        "train_acc\n",
        "\n",
        "test_pred = torch.argmax(model_dnn(x_test.to(torch.float32).to(device)), dim=1)\n",
        "zero_mask = (test_pred==y_test.to(device))\n",
        "# Count the number of zeros\n",
        "test_acc = zero_mask.sum().item()/len(y_test)\n",
        "test_acc"
      ],
      "metadata": {
        "id": "lO7jL6mJ0RYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a132446-7081-4691-f78e-d4a356e6403e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train Loss 1.0755  valid loss 1.0905  valid acc 0.40634755463059313\n",
            "Epoch 10 Train Loss 0.7480  valid loss 1.2152  valid acc 0.4474505723204995\n",
            "Epoch 20 Train Loss 0.5929  valid loss 1.2686  valid acc 0.4308012486992716\n",
            "Epoch 30 Train Loss 0.3598  valid loss 1.4383  valid acc 0.45265348595213317\n",
            "Epoch 40 Train Loss 0.2330  valid loss 1.8477  valid acc 0.4625390218522373\n",
            "Epoch 50 Train Loss 0.1498  valid loss 2.5840  valid acc 0.41467221644120705\n",
            "Epoch 60 Train Loss 0.1034  valid loss 3.5580  valid acc 0.41467221644120705\n",
            "Epoch 70 Train Loss 0.0173  valid loss 4.3319  valid acc 0.43548387096774194\n",
            "Epoch 80 Train Loss 0.0402  valid loss 3.3829  valid acc 0.441207075962539\n",
            "Epoch 90 Train Loss 0.0081  valid loss 5.3853  valid acc 0.43184183142559834\n",
            "Early stopping at epoch 90 because loss is below 0.01\n",
            "tensor(3.4058, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.577962577962578"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_test.detach(), test_pred.detach().cpu().numpy(), average='macro')"
      ],
      "metadata": {
        "id": "S_neZ0Jm69XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c8f04b-5241-4aab-9092-df442457e981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5664371998723438"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import csv\n",
        "\n",
        "def find_common_sentences(file1_path, file2_path):\n",
        "    common_sentences = set()\n",
        "\n",
        "\n",
        "    with open(file1_path, 'r', newline='') as file1:\n",
        "        reader1 = csv.reader(file1, delimiter='\\t')\n",
        "        for row in reader1:\n",
        "            if len(row) > 0:\n",
        "                common_sentences.add(row[0])\n",
        "\n",
        "\n",
        "    common_count = 0\n",
        "    with open(file2_path, 'r', newline='') as file2:\n",
        "        reader2 = csv.reader(file2, delimiter='\\t')\n",
        "        for row in reader2:\n",
        "            if len(row) > 0 and row[0] in common_sentences:\n",
        "                common_count += 1\n",
        "\n",
        "    return common_count\n",
        "\n",
        "\n",
        "file1_path = 'NLP_ass_train.tsv'\n",
        "file2_path = 'NLP_ass_test.tsv'\n",
        "\n",
        "\n",
        "common_count = find_common_sentences(file1_path, file2_path)\n",
        "file3_path = 'NLP_ass_valid.tsv'\n",
        "\n",
        "\n",
        "common_count1 = find_common_sentences(file1_path, file3_path)\n",
        "\n",
        "\n",
        "print(f\"Number of common sentences between test and train datasets: {common_count}\")\n",
        "print(f\"Number of common sentences between validation and train datasets: {common_count1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGEwoN9U5bfr",
        "outputId": "c22c9d2a-4a7f-4604-d414-01e68d9a3c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of common sentences between test and train datasets: 6\n",
            "Number of common sentences between validation and train datasets: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vpdeBF4_6jwo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}